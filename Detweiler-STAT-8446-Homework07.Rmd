---
title: "Homework 7"
author: "Brian Detweiler"
date: "March 29, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(TSA)
library(forecast)
library(ggplot2)
library(zoo)
library(scales)
```

# 1. Consider the AR(2) model: $Y_t = c + \phi_1 Y_{t - 1} + \phi_2 Y_{t - 2} + e_t$. If we have a series of length 100, with $r_1 = 0.5, r_2 = -0.2, r_3 = 0.1, \overline{y} = 4$ and $s^2 = 6$, use the method of moments estimators to calculate estimates of $c, \phi_1, \phi_2$ and $\sigma_e^2$ manually.

From 7.1.2 in the book, we have

$$
\begin{split}
  \hat{\phi_1} &= \frac{r_1 (1 - r_2)}{1 - r_1^2} \\
  &= \frac{0.5 (1 - (-0.2))}{1 - (0.5)^2} \\
  &= `r (0.5 * (1 - (-0.2))) / (1 - (0.5)^2)` \\
  \hat{\phi_2} &= \frac{r_2 - r_1^2}{1 - r_1^2} \\
  &= \frac{-0.2 - (0.5)^2}{1 - (0.5)^2} \\
  &= `r (-0.2 - (0.5)^2) / (1 - (0.5)^2)` \\
\end{split}
$$

From 7.1.8 in the book, we can calculate the noise variance,

$$
\begin{split}
  \hat{\sigma}_e^2 &= (1 - \hat{\phi_1} r_1 - \hat{\phi_2} r_2) s^2 \\
  &= (1 - 0.8 (0.5) - (-0.6) (-0.2)) 6 \\
  &= `r (1 - 0.8 * (0.5) - (-0.6) * (-0.2)) * 6` \\
\end{split}
$$

With Method of Moments, we let $\hat{\mu} = \overline{y}$, so 

$$
\begin{split}
  (Y_t - \mu) &= c + \phi_1 (Y_{t - 1} - \mu) + \phi_2 (Y_{t - 2} - \mu) + e_t \\
  (Y_t - \overline{y}) &= c + \phi_1 (Y_{t - 1} - \overline{y}) + \phi_2 (Y_{t - 2} - \overline{y}) + e_t \\
  c &= (Y_t - \overline{y}) - \phi_1 (Y_{t - 1} - \overline{y}) - \phi_2 (Y_{t - 2} - \overline{y}) - e_t \\
  \hat{c} &= (Y_t - 4) - \hat{\phi_1} (Y_{t - 1} - 4) - \hat{\phi_2} (Y_{t - 2} - 4) - e_t \\
  \hat{c} &= (Y_t - 4) - (0.8) (Y_{t - 1} - 4) - (-0.6) (Y_{t - 2} - 4) - e_t \\
\end{split}
$$

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 2. Assume that the following data arise from a stationary process: 8, 7, 6, 9, 7. Calculate method-of-moments estimates of $\mu, \gamma_0$, and $\rho_1$.

We only have the 5 data points to go off of, so we'll calculate the sample mean and sample variance and use that to get our estimates of the true paramaters.

```{r}
dat <- c(8, 7, 6, 9, 7)
```
$$
\begin{split}
  \overline{y} &= `r mean(dat)` \\
  s_k &= \frac{1}{n} \sum_{i = 1}^{n - k} (y_i - \overline{y})(y_{i + k} - \overline{y}) \\
  \hat{\gamma_0} &= s_0 = \frac{1}{5} \sum_{i = 1}^{5} (y_i - \overline{y})(y_{i} - \overline{y}) \\
  &= \frac{(8 - 7.4)^2 + (7 - 7.4)^2 + (6 - 7.4)^2 + (9 - 7.4)^2 + (7 - 7.4)^2}{5} \\
  &= \frac{`r (8 - 7.4)^2 + (7 - 7.4)^2 + (6 - 7.4)^2 + (9 - 7.4)^2 + (7 - 7.4)^2`}{5} \\
  &= `r ((8 - 7.4)^2 + (7 - 7.4)^2 + (6 - 7.4)^2 + (9 - 7.4)^2 + (7 - 7.4)^2) / 5` \\
  s_1 &= \frac{1}{5} \sum_{i = 1}^{4} (y_i - \overline{y})(y_{i + 1} - \overline{y}) \\
  &= \frac{(8 - 7.4)(7 - 7.4) + (7 - 7.4)(6 - 7.4) + (6 - 7.4)(9 - 7.4) + (9 - 7.4)(7 - 7.4)}{5} \\
  &= \frac{`r (8 - 7.4)*(7 - 7.4) + (7 - 7.4)*(6 - 7.4) + (6 - 7.4)*(9 - 7.4) + (9 - 7.4)*(7 - 7.4)`}{5} \\
  &= `r ((8 - 7.4)*(7 - 7.4) + (7 - 7.4)*(6 - 7.4) + (6 - 7.4)*(9 - 7.4) + (9 - 7.4)*(7 - 7.4)) / 5`  \\
  \hat{\rho_1} &= r_1 = \frac{s_1}{s_0} = \frac{`r ((8 - 7.4)*(7 - 7.4) + (7 - 7.4)*(6 - 7.4) + (6 - 7.4)*(9 - 7.4) + (9 - 7.4)*(7 - 7.4)) / 5`}{ `r ((8 - 7.4)^2 + (7 - 7.4)^2 + (6 - 7.4)^2 + (9 - 7.4)^2 + (7 - 7.4)^2) / 5`} \\
  &= `r (((8 - 7.4)*(7 - 7.4) + (7 - 7.4)*(6 - 7.4) + (6 - 7.4)*(9 - 7.4) + (9 - 7.4)*(7 - 7.4)) ) / (((8 - 7.4)^2 + (7 - 7.4)^2 + (6 - 7.4)^2 + (9 - 7.4)^2 + (7 - 7.4)^2) )` \\
\end{split}
$$

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 3. Simulate (use your NUID as the seed) an MA(1) series with $\theta = 0.7$ and $n = 36$. Read pages 443-444 of the text book to learn the \textsf{R} functions that compute the following estimators.

```{r}
set.seed(48548493)
n <- 36
Y_t <- arima.sim(n=n, model=list(ma=c(-0.7)))
```

## (a) Find the method of moments estimator of $\theta$. (Hint: the user-created function \texttt{estimate.ma1.mom})

```{r}
estimate.ma1.mom <- function(x) {
  r <- acf(x, plot=F)$acf[1]
  if (abs(r) < 0.5) {
    return(-(-1 + sqrt(1 - 4 * r^2)) / (2 * r))
  } else {
    return(NA)
  }
}

theta1.hat <- estimate.ma1.mom(Y_t)

```

The MoM estimate is $\hat{\theta_1}$ =  `r theta1.hat`.

## (b) Find the (conditional) least squares estimator of $\theta$. (Hint: the \texttt{arima} function with \texttt{method="CSS"})

```{r}
a <- arima(x=Y_t, order = c(0,0,1), method="CSS")
theta1.hat <- a$coef[[1]]
```

The conditional sum of squares estimate for $\theta_1$ is `r theta1.hat`.

## (c) Find the maximum likelihood estimator of $\theta$. (Hint: the \texttt{arima} function with \texttt{method="ML"})

```{r}
a <- arima(x=Y_t, order = c(0,0,1), method="ML")
theta1.hat <- a$coef[[1]]
```

The conditional sum of squares estimate for $\theta_1$ is `r theta1.hat`.

## (d) Compare the three estimators, which is closest to the actual $\theta$?

The Maximum Likelihood is the closest, and performs best with smaller sample sizes.

## (e) Generate 1000 simulated series and repeat parts (a) - (d). Make three histograms for the three estimators resp ectively. Comment on the results.

```{r}
n <- 1000
Y_t <- arima.sim(n=n, model=list(ma=c(-0.7)))

theta1.hat <- estimate.ma1.mom(Y_t)
```

Method of Moments: $\hat{\theta}$ = `r theta1.hat`

```{r}
a <- arima(x=Y_t, order = c(0,0,1), method="CSS")
theta1.hat <- a$coef[[1]]
```

Conditional Sum of Squares: $\hat{\theta}$ = `r theta1.hat`

```{r}
a <- arima(x=Y_t, order = c(0,0,1), method="ML")
theta1.hat <- a$coef[[1]]
```

Maximum Likelihood: $\hat{\theta}$ =  `r theta1.hat`

With more data, the Conditional Sum of Squares performs slightly better than the Maximum Likelihood estimator, but both are fairly close.

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak


# 4. Three data files \texttt{deere1}, \texttt{deere2}, \texttt{deere3} from the \texttt{TSA} package contains different number of consecutive values for the amount of deviation from some specific target values by three machining process. For each of the three data sets, do the following:

```{r}
data(deere1)
data(deere2)
data(deere3)
```

## (a) Plot the time series and comment on its appearance. Would a stationary model seem to be appropriate?

```{r}
plot(deere1, main="deere1")
abline(h=0)
```

This looks stationary except for the one outlier at time `r which(deere1 == 30)`. We could replace this with the mean, `r mean(deere1)` and treat it as stationary.

```{r}
deere1.mod <- deere1
deere1.mod[27] <- mean(deere1)
plot(deere1.mod, main="deere1 with outlier replaced")
abline(h=0)
```

```{r}
plot(deere2, main="deere2")
abline(h=0)
```

If we drop the first 6 observations, this time series becomes more stationary.

```{r}
plot(as.ts(deere2[7:length(deere2)]), main="deere2 with first 6 dropped")
abline(h=0)
```

```{r}
plot(deere3, main="deere3")
abline(h=0)
```

Although there appears to be an outlier at time `r which(deere3 == -5750)`, this appears to be close to stationary.

```{r}
deere3.mod <- deere3
deere3.mod[54] <- mean(deere3)
plot(deere3.mod, main="deere3 with outlier removed")
abline(h=0)
```

## (b) Discuss if any log or power transformation should be applied to the series to improve the normality.

Log transformations would not be possible here, because we have negative values. This is even problematic for Box-Cox transformations, since a positive constant must be first added to the negative numbers before applying the transformation. For this reason, transformations are not used.

## (c) Perform the (augmented) Dickey-Fuller test on the series. Decide if the series should be differenced to get a stationary model.

```{r}
adf.test(deere1)
adf.test(deere1.mod)
```

Performing the augmented Dickey-Fuller test on the first series and it's modified version show that both appear to be stationary. The modified series has a lower p-value, as we would expect.

```{r}
adf.test(deere2)
```

With $p << 0.05$, the \texttt{deere2} series appears to be stationary.

```{r}
adf.test(deere3)
```

Similarly, $p << 0.05$ for the \texttt{deere3} series, so it also appears to be stationary.

\pagebreak

## (d) Display the sample \texttt{ACF}, \texttt{PACF} and \texttt{EACF} for the series, and select tentative orders for an ARMA model.

### Time series \texttt{deere1}

```{r, fig.height=4, fig.width=5, fig.align='center'}
acf(deere1)
pacf(deere1)
eacf(deere1)
```

Based on the ACF, PACF, and EACF, I would recommend an ARMA(2, 2) model for this.

### Time series \texttt{deere2}

```{r, fig.height=4, fig.width=5, fig.align='center'}
acf(deere2)
pacf(deere2)
eacf(deere2)
```

Based on the ACF, PACF, and EACF, I would recommend an ARMA(1, 2) model for this.

### Time series \texttt{deere3}

```{r, fig.height=4, fig.width=5, fig.align='center'}
acf(deere3)
pacf(deere3)
eacf(deere3)
```

Based on the ACF, PACF, and EACF, I would recommend an ARMA(1, 2) model for this.

## (e) Use the best subsets ARMA approach to specify a model for the series. Compare the result with what you discovered in part (d).

### Time series \texttt{deere1}

```{r, fig.height=4, fig.width=5, fig.align='center'}
res <- armasubsets(y=deere1, nar=14, nma=14, ar.method='ols')
plot(res)
```

This looks to be an AR(2).

### Time series \texttt{deere2}

```{r, fig.height=4, fig.width=5, fig.align='center'}
res <- armasubsets(y=deere2, nar=14, nma=14, ar.method='ols')
plot(res)
```

This looks to be an ARMA(1, 1).

### Time series \texttt{deere3}

```{r, fig.height=4, fig.width=5, fig.align='center'}
res <- armasubsets(y=deere3, nar=14, nma=14, ar.method='ols')
plot(res)
```

This looks to be an ARMA(2, 1).

\begin{flushright}
  $\blacksquare$
\end{flushright}
